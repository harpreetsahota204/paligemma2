{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting existing directory '/home/harpreet/fiftyone/quickstart'\n",
      "Downloading dataset to '/home/harpreet/fiftyone/quickstart'\n",
      "Downloading dataset...\n",
      " 100% |████|  187.5Mb/187.5Mb [488.3ms elapsed, 0s remaining, 384.0Mb/s]      \n",
      "Extracting dataset...\n",
      "Parsing dataset metadata\n",
      "Found 200 samples\n",
      "Dataset info written to '/home/harpreet/fiftyone/quickstart/info.json'\n",
      "Loading existing dataset 'smol_start'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo \n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "dataset = foz.load_zoo_dataset(\n",
    "    \"quickstart\",\n",
    "    max_samples=2,\n",
    "    dataset_name=\"smol_start\", \n",
    "    shuffle=True,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d328a2a448434a318b9436fc82d73fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<loc0001><loc0000><loc1010><loc1020><seg010><seg090><seg090><seg000><seg054><seg082><seg082><seg027><seg035><seg082><seg082><seg027><seg023><seg090><seg082><seg010>\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    PaliGemmaProcessor,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "from transformers.image_utils import load_image\n",
    "import torch\n",
    "\n",
    "model_id = \"google/paligemma2-3b-mix-224\"\n",
    "\n",
    "url = dataset.first().filepath\n",
    "\n",
    "image = load_image(url)\n",
    "\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda\").eval()\n",
    "processor = PaliGemmaProcessor.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "prompt = \"\"\"<image> segment hen; grass; baby chick\\n\"\"\"\n",
    "model_inputs = processor(\n",
    "    text=prompt, \n",
    "    images=image, \n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    generation = model.generate(**model_inputs, max_new_tokens=3092)\n",
    "    generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "    print(decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<loc0001><loc0243><loc0963><loc0666><seg074><seg074><seg069><seg038><seg104><seg056><seg023><seg030><seg070><seg099><seg099><seg073><seg014><seg087><seg019><seg118>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(task: str, prompt: str | list | None = None) -> str:\n",
    "    \"\"\"Create a formatted prompt string for PaliGemma2 vision-language tasks.\n",
    "    \n",
    "    Args:\n",
    "        task: The vision task to perform. Must be one of:\n",
    "            - \"cap\", \"caption\", \"describe\": Captioning with different detail levels\n",
    "            - \"ocr\": Optical character recognition\n",
    "            - \"answer\", \"question\": Visual QA tasks\n",
    "            - \"detect\": Object detection\n",
    "            - \"segment\": Instance segmentation\n",
    "        prompt: Main task input. Could be:\n",
    "            - Question for \"answer\" task\n",
    "            - Answer for \"question\" task\n",
    "            - Objects for \"detect\" task (string or list, joined with \" ; \")\n",
    "            - Object for \"segment\" task\n",
    "    \n",
    "    Returns:\n",
    "        str: Formatted prompt string ready for model input\n",
    "        \n",
    "    Examples:\n",
    "        >>> create_prompt(\"detect\", [\"car\", \"person\"])\n",
    "        '<image> detect car ; person'\n",
    "        >>> create_prompt(\"answer\", \"What color is the car?\")\n",
    "        '<image> answer en What color is the car?'\n",
    "        >>> create_prompt(\"ocr\")\n",
    "        '<image> ocr'\n",
    "    \"\"\"\n",
    "    # Handle OCR as special case with no parameters\n",
    "    if task == \"ocr\":\n",
    "        return \"<image> ocr\"\n",
    "        \n",
    "    # Process list inputs for detection/segmentation\n",
    "    if isinstance(prompt, (list, tuple)):\n",
    "        prompt = \" ; \".join(str(p) for p in prompt)\n",
    "    \n",
    "    # Build task-specific prompt\n",
    "    if task in [\"cap\", \"caption\", \"describe\"]:\n",
    "        return f\"<image> {task} en\"\n",
    "    elif task in [\"answer\", \"question\"]:\n",
    "        return f\"<image> {task} en {prompt}\"\n",
    "    elif task in [\"detect\", \"segment\"]:\n",
    "        return f\"<image> {task} {prompt}\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<image> caption en'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt(task=\"caption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<image> answer en What color is the car?'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt(task=\"answer\", prompt=\"What color is the car?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<image> detect car; boat; house'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt(task=\"detect\", prompt=\"car; boat; house\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<image> detect car ; boat ; house'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt(task=\"detect\", prompt=[\"car\", \"boat\", \"house\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<image> segment car ; boat ; house'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_prompt(task=\"segment\", prompt=[\"car\", \"boat\", \"house\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
